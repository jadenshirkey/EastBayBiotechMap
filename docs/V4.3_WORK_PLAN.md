# V4.3 Framework Implementation Plan
**Complete Dependency-Ordered Work Plan for GitHub Issues & PRs**

**Project:** Bay Area Biotech Map
**Version:** 4.3
**Date:** 2025-11-15
**Status:** Implementation plan for V4.3 Framework

---

## Executive Summary

**Total Issues:** 27
**Total Estimated Hours:** ~53h (excluding variable manual review time)
**Implementation Scope:** 6 pipeline stages (A→F) + infrastructure + documentation
**Critical Fixes:**
- BPG source URL (state-ca-all-geo.php)
- Website field capture (external link preservation)
- eTLD+1 deduplication with domain-reuse detection
- Gated validation with deterministic scoring
- Staging-only writes (no direct writes to final/)

**Key Architectural Changes:**
- **BPG-first:** Treat BioPharmGuy Website as canonical ground truth
- **Two-path enrichment:** Path A (pure Python + gated validation) for companies with Website; Path B (Anthropic structured outputs) for companies without Website
- **Centralized geofence:** Single 9-county + city whitelist module
- **Staging → promotion flow:** All writes to working/ during pipeline; final/ only written by promotion script after QC gates pass

---

## Dependency Flow Diagram

```
┌─────────────────────────────────────────────────────────────┐
│ PHASE 0: Infrastructure (Issues #1-3)                       │
│ ├─ Geography module (9-county, city whitelist)             │
│ ├─ Helpers (eTLD+1, scoring, multi-tenant)                 │
│ └─ Test framework + red-team dataset (30 companies)        │
└─────────────────────────────────────────────────────────────┘
                           │
                           ▼
┌─────────────────────────────────────────────────────────────┐
│ PHASE 1: PR-1 Stage-A Extractor (Issues #4-6)              │
│ ├─ Switch to CA-wide source (state-ca-all-geo.php)         │
│ ├─ Capture Website field (external link)                   │
│ └─ Add caching, validation, error handling                 │
└─────────────────────────────────────────────────────────────┘
                           │
                           ▼
┌─────────────────────────────────────────────────────────────┐
│ PHASE 2: PR-2 Stage-B Merge/Geofence (Issues #7-10)        │
│ ├─ eTLD+1 dedupe + domain-reuse detection                  │
│ ├─ Centralized geofence (late filtering)                   │
│ ├─ Aggregator denylist + staging output                    │
│ └─ Preserve BPG Website (don't blank)                      │
└─────────────────────────────────────────────────────────────┘
                           │
         ┌─────────────────┴─────────────────┐
         ▼                                   ▼
┌──────────────────────────┐  ┌──────────────────────────────┐
│ PR-3 Stage-C Path A      │  │ PR-4 Stage-C Path B          │
│ (Issues #11-14)          │  │ (Issues #15-19)              │
│ ├─ Gated validation      │  │ ├─ Anthropic tool defs       │
│ ├─ Deterministic scoring │  │ ├─ JSON schema + prompt      │
│ ├─ Cost instrumentation  │  │ ├─ Tool use controller loop  │
│ ├─ Error handling        │  │ ├─ Acceptance threshold      │
│ └─ Path A/B routing      │  │ └─ Merge Path A + Path B     │
└──────────────────────────┘  └──────────────────────────────┘
         │                                   │
         └─────────────────┬─────────────────┘
                           ▼
┌─────────────────────────────────────────────────────────────┐
│ PHASE 4: Stages D/E/F (Issues #20-24)                      │
│ ├─ Classification (methodology decision tree)              │
│ ├─ Focus Areas extraction (≤200 chars)                     │
│ ├─ Automated validators (6 validators)                     │
│ ├─ Manual review queues (Tier 1/2 spot-checks, Tier 4 all) │
│ └─ Promotion to final/ (ONLY after validators pass)        │
└─────────────────────────────────────────────────────────────┘
                           │
                           ▼
┌─────────────────────────────────────────────────────────────┐
│ PHASE 5: Documentation (Issues #25-27)                     │
│ ├─ Update METHODOLOGY.md (validation strategy, tiers)      │
│ ├─ Update README.md (V4.3 features, data quality)          │
│ └─ Create V4.3_IMPLEMENTATION_GUIDE.md (runbook)           │
└─────────────────────────────────────────────────────────────┘
```

---

## Phase 0: Infrastructure (7 hours)

### Issue #1: Create centralized geography module

**Title:** Create config/geography.py with canonical Bay Area definitions

**Rationale:** Current scripts have inconsistent geofence logic spread across files. Methodology defines 9-county Bay Area; centralize to ensure consistency and enable reuse.

**Acceptance Criteria:**
- Module exports BAY_COUNTIES (9 counties), CITY_WHITELIST (~50 cities), SF_LATLNG, BAY_RADIUS_M
- Function: `is_in_bay_area(city, county) → bool`
- Function: `is_within_radius(lat, lng, center, radius_m) → bool`
- Unit tests cover all counties and cities

**Concrete Changes:**
- ADD: `config/geography.py` (new module)
- ADD: `tests/test_geography.py` (unit tests)

**Test Plan:**
- Unit: Verify all 9 counties recognized
- Unit: Verify ~50 cities in whitelist
- Unit: Test edge cases (South SF vs South San Francisco)
- Unit: Test radius calculation for known coordinates

**Rollback Plan:** Delete config/geography.py; scripts haven't adopted it yet

**Estimated Hours:** 2h

**Labels:** V4.3, infrastructure

---

### Issue #2: Create helper utilities module

**Title:** Create utils/helpers.py with deterministic scoring and validation functions

**Rationale:** Path A and Path B need shared utilities for eTLD+1 extraction, brand token parsing, name similarity, aggregator detection, multi-tenant handling.

**Acceptance Criteria:**
- Function: `etld1(url) → str` (using tldextract)
- Function: `brand_token_from_etld1(domain) → str`
- Function: `is_aggregator(url) → bool` (denylisted domains)
- Function: `name_similarity(a, b) → float [0,1]`
- Function: `is_multi_tenant(address) → bool`
- Function: `validate_multi_tenant_match(...) → bool`
- AGGREGATOR_ETLD1 constant with 10+ domains
- INCUBATOR_ADDRESSES constant with 3+ known addresses
- Unit tests achieve 100% coverage

**Concrete Changes:**
- ADD: `utils/helpers.py` (new module)
- ADD: `requirements.txt` entries: tldextract, textdistance
- ADD: `tests/test_helpers.py` (unit tests)

**Test Plan:**
- Unit: etld1() handles https, www, subdomains correctly
- Unit: brand_token matches examples (gene.com → gene)
- Unit: Aggregator detection catches all denylisted domains
- Unit: Name similarity scores known pairs correctly
- Unit: Multi-tenant detection for known incubator addresses

**Rollback Plan:** Delete utils/helpers.py; no scripts depend on it yet

**Estimated Hours:** 3h

**Labels:** V4.3, infrastructure

---

### Issue #3: Setup test framework and red-team dataset

**Title:** Create test framework with 30-company red-team validation set

**Rationale:** Need systematic testing before full runs. Red-team set covers edge cases: clear wins, incubator tenants, alias brands, city edges, aggregator-only, out-of-scope distractors.

**Acceptance Criteria:**
- File: `tests/fixtures/red_team_companies.csv` with 30 rows
- Categories: 10 clear wins, 5 incubator tenants, 5 alias/unusual domains, 5 city edges, 3 aggregator-only, 2 out-of-scope
- pytest configuration: tests/ directory, coverage reporting
- CI/CD: GitHub Actions workflow for automated test runs

**Concrete Changes:**
- ADD: `tests/fixtures/red_team_companies.csv`
- ADD: `tests/conftest.py` (pytest fixtures)
- ADD: `.github/workflows/test.yml` (CI workflow)
- ADD: `pytest.ini` or `pyproject.toml` (pytest config)

**Test Plan:**
- Validation: Red-team set includes all required categories
- Integration: pytest discovers and runs all tests
- CI: GitHub Actions runs on PR creation

**Rollback Plan:** Delete test infrastructure; doesn't affect production code

**Estimated Hours:** 2h

**Labels:** V4.3, infrastructure, testing

---

## Phase 1: PR-1 BPG Extractor [Stage-A] (4 hours)

**Dependencies:** Phase 0 (helpful but not blocking)

### Issue #4: Switch BPG source to CA-wide and capture Website field

**Title:** [Stage-A] Switch BPG extractor to state-ca-all-geo.php and capture Website

**Rationale:** Current extractor uses Northern California page (misses many Bay Area companies) and doesn't save the external Website link (loses ground truth). V4.3 requires CA-wide extraction with Website field to maximize coverage and preserve canonical domain data.

**Acceptance Criteria:**
- Source URL changed to: `https://biopharmguy.com/links/state-ca-all-geo.php`
- Output CSV includes columns: Company Name, Website, City, Focus Area, Source URL, Notes
- Website column populated from external link (second `<a>` in BPG row) when present
- NO Bay Area filtering in extractor (geofence happens in Stage B)
- Extraction count logged: total rows, rows with Website, rows without Website
- Percentage coverage: (rows with Website / total) × 100

**Concrete Changes:**
- EDIT: `scripts/extract_biopharmguy_companies.py`
  - Change BIOPHARMGUY_URL constant to state-ca-all-geo.php
  - Remove any Bay Area city filtering logic
  - Add Website field extraction from external `<a href>`
  - Add extraction statistics logging
- EDIT: `data/working/bpg_ca_raw.csv` (output file, new schema)

**Test Plan:**
- Unit: Test HTML parsing with mock BPG page structure
- Unit: Test Website extraction from various row formats
- Unit: Verify NO filtering occurs (all CA companies included)
- Integration: Run on live BPG page, verify output CSV schema
- Validation: Check percentage with Website ≥ 70%
- Red-team: Verify known companies (Genentech, BioMarin, Gilead) extracted with Website

**Rollback Plan:**
- Revert scripts/extract_biopharmguy_companies.py to previous commit
- Delete data/working/bpg_ca_raw.csv
- No impact on data/final/ (staging only)

**Estimated Hours:** 2h

**Labels:** V4.3, Stage-A

---

### Issue #5: Add HTML caching and respectful scraping

**Title:** [Stage-A] Add disk cache, user agent, and rate limiting to BPG extractor

**Rationale:** Minimize re-fetching BPG page between runs; respect website ToS; enable offline development and testing.

**Acceptance Criteria:**
- HTML cached to `data/cache/bpg_ca_YYYYMMDD.html`
- Cache used if < 7 days old
- User-Agent header: "BayAreaBiotechMap/4.3 (Research Project; contact@example.com)"
- Rate limiting: 2-second delay if multiple fetches needed
- Exponential backoff on HTTP errors: 0.5s, 1s, 2s (3 retries)

**Concrete Changes:**
- EDIT: `scripts/extract_biopharmguy_companies.py`
  - Add cache_html() and load_cached_html() functions
  - Add USER_AGENT constant
  - Add retry logic with backoff
- ADD: `data/cache/` directory (gitignored)
- EDIT: `.gitignore` (add data/cache/)

**Test Plan:**
- Unit: Verify cache saves and loads correctly
- Unit: Verify cache expiration after 7 days
- Unit: Test retry logic with mock HTTP errors
- Integration: First run fetches, second run uses cache

**Rollback Plan:** Remove caching code; extractor still works (just fetches every time)

**Estimated Hours:** 1h

**Labels:** V4.3, Stage-A

---

### Issue #6: Add extraction validation and error handling

**Title:** [Stage-A] Add validation checks and error handling to extractor

**Rationale:** Catch extraction failures early; provide actionable error messages; ensure data quality from source.

**Acceptance Criteria:**
- Validate: Output CSV has > 0 rows
- Validate: All rows have Company Name (non-empty)
- Validate: Website field is valid URL format or empty string
- Validate: No duplicate Company Names in output
- Error handling: Graceful failure if BPG page structure changed
- Logging: INFO for progress, WARNING for anomalies, ERROR for failures

**Concrete Changes:**
- EDIT: `scripts/extract_biopharmguy_companies.py`
  - Add validate_extraction_output() function
  - Add structured logging (Python logging module)
  - Add schema validation for CSV output

**Test Plan:**
- Unit: Test validation catches empty output
- Unit: Test validation catches invalid URLs
- Unit: Test validation catches duplicates
- Integration: Run on real BPG page, verify all validations pass

**Rollback Plan:** Remove validation code; core extraction still works

**Estimated Hours:** 1h

**Labels:** V4.3, Stage-A

---

## Phase 2: PR-2 Merge & Geofence [Stage-B] (6 hours)

**Dependencies:** Phase 0 (Issues #1, #2), PR-1 (Issue #4)

### Issue #7: Implement eTLD+1 deduplication and domain-reuse detection

**Title:** [Stage-B] Add eTLD+1 deduplication with domain-reuse conflict reporting

**Rationale:** Current merge uses simple name matching. V4.3 requires eTLD+1 + normalized name dedupe to catch domain conflicts. Domain-reuse report blocks promotion until resolved (prevents multiple companies claiming same domain).

**Acceptance Criteria:**
- Import utils/helpers.py for etld1() function
- Extract eTLD+1 from all Website values using tldextract
- Normalize company names (lowercase, strip suffixes like Inc/LLC, remove punctuation)
- Dedupe by (eTLD+1, normalized_name) tuple; prefer row with most complete fields
- Generate domain_reuse_report.txt: list any eTLD+1 claimed by >1 company
- BLOCK promotion (exit with error) if unresolved duplicates detected
- Allow-list exceptions: gene.com (multi-brand) configurable

**Concrete Changes:**
- EDIT: `scripts/merge_company_sources.py`
  - Import from utils.helpers import etld1
  - Add normalize_company_name() function
  - Replace simple name dedupe with (eTLD+1, normalized_name) logic
  - Add generate_domain_reuse_report() function
  - Add ALLOWLIST_DOMAINS constant
- ADD: `data/working/domain_reuse_report.txt` (generated output)

**Test Plan:**
- Unit: Test etld1() handles various URL formats
- Unit: Test normalize_name() strips suffixes correctly
- Unit: Test dedupe logic prefers most complete row
- Unit: Test domain-reuse detection with mock conflicts
- Unit: Test allow-list bypasses known multi-brand domains
- Integration: Run on bpg_ca_raw.csv + wikipedia + existing, verify no duplicates
- Red-team: Verify Genentech variants consolidated to single entry

**Rollback Plan:**
- Revert scripts/merge_company_sources.py to previous commit
- Delete data/working/domain_reuse_report.txt
- No impact on data/final/

**Estimated Hours:** 2h

**Labels:** V4.3, Stage-B

---

### Issue #8: Integrate centralized geofence module and apply late filtering

**Title:** [Stage-B] Replace early Bay Area filtering with late geofence using geography module

**Rationale:** Current merge filters by city name early (before enrichment). V4.3 requires geofencing AFTER dedupe using centralized 9-county + city whitelist module. This maximizes coverage and centralizes scope logic.

**Acceptance Criteria:**
- Import from config.geography import BAY_COUNTIES, CITY_WHITELIST, is_in_bay_area
- Remove any early city name filtering
- Apply geofence AFTER deduplication step
- Accept row if: City in CITY_WHITELIST OR County in BAY_COUNTIES
- Log: total CA rows, rows passing geofence, rows filtered out
- Output only Bay Area companies to companies_merged.csv

**Concrete Changes:**
- EDIT: `scripts/merge_company_sources.py`
  - Import from config.geography
  - Remove early city filtering logic
  - Add apply_geofence() function after dedupe
  - Add geofence statistics logging
- DEPENDS: Issue #1 (geography module must exist)

**Test Plan:**
- Unit: Test geofence accepts all 9 counties
- Unit: Test geofence accepts all ~50 whitelisted cities
- Unit: Test geofence rejects Davis, Sacramento, San Diego
- Integration: Verify merge output contains only Bay Area companies
- Red-team: Verify Genentech (South SF), Gilead (Foster City) pass geofence

**Rollback Plan:** Revert merge script; geography module remains (unused)

**Estimated Hours:** 2h

**Labels:** V4.3, Stage-B

---

### Issue #9: Implement aggregator denylist and staging-only output

**Title:** [Stage-B] Add aggregator domain denylist and change output to staging directory

**Rationale:** Current merge writes to data/final/ directly. V4.3 requires staging output only (final/ written only during promotion). Also need to reset Website='' for aggregator/parking domains to force Path B enrichment.

**Acceptance Criteria:**
- Import from utils.helpers import is_aggregator, AGGREGATOR_ETLD1
- Check each Website against aggregator denylist
- If aggregator detected: set Website='' (forces Path B), log WARNING
- Output to `data/working/companies_merged.csv` (NOT data/final/)
- Add output path validation (must be in working/ directory)
- Log: total aggregators found, companies routed to Path B

**Concrete Changes:**
- EDIT: `scripts/merge_company_sources.py`
  - Import from utils.helpers import is_aggregator
  - Add check_and_reset_aggregators() function
  - Change output path from data/final/companies.csv to data/working/companies_merged.csv
  - Add path validation assertion
- DEPENDS: Issue #2 (helpers module with AGGREGATOR_ETLD1)

**Test Plan:**
- Unit: Test is_aggregator() catches all denylisted domains
- Unit: Test Website reset to '' for aggregator matches
- Unit: Test output path validation rejects data/final/ paths
- Integration: Verify no aggregator domains in merged output
- Integration: Verify output file in working/ not final/

**Rollback Plan:**
- Revert merge script
- Delete data/working/companies_merged.csv
- data/final/ unchanged

**Estimated Hours:** 1h

**Labels:** V4.3, Stage-B

---

### Issue #10: Preserve BPG Website field (don't reset to blank)

**Title:** [Stage-B] Preserve BPG Website field as canonical ground truth

**Rationale:** Current merge sets BPG Website to '' (loses ground truth). V4.3 treats BPG Website as canonical for Path A cross-validation. Only reset to '' if aggregator or invalid format.

**Acceptance Criteria:**
- Keep BPG Website field unchanged (unless aggregator)
- Add source tracking: Validation_Source column ("BPG", "Wikipedia", "Existing")
- Prioritize BPG Website over Wikipedia/Existing when deduping
- Log: BPG rows with Website, Wikipedia rows with Website, merged coverage

**Concrete Changes:**
- EDIT: `scripts/merge_company_sources.py`
  - Remove logic that blanks BPG Website
  - Add Validation_Source column to output
  - Update dedupe preference logic (BPG > Existing > Wikipedia)
  - Add coverage statistics

**Test Plan:**
- Unit: Test BPG Website preserved during merge
- Unit: Test Validation_Source correctly assigned
- Unit: Test dedupe prefers BPG over other sources
- Integration: Verify output has Validation_Source column
- Red-team: Verify Genentech Website preserved from BPG

**Rollback Plan:** Revert merge script changes

**Estimated Hours:** 1h

**Labels:** V4.3, Stage-B

---

## Phase 3: PR-3 Path A Enrichment [Stage-C PathA] (9 hours)

**Dependencies:** Phase 0 (Issues #1, #2), PR-2 (Issues #7-10)

**Note:** Can develop in parallel with PR-4 after PR-2 completes

### Issue #11: Implement Path A deterministic scoring and validation gates

**Title:** [Stage-C PathA] Replace first-result-wins with gated validation and deterministic scoring

**Rationale:** Current enrich_with_google_maps.py accepts first Google Places result without validation. V4.3 requires hard gates (geofence, business-type, multi-tenant) and deterministic confidence scoring to ensure quality.

**Acceptance Criteria:**
- Build biasable query: "{brand_token} {City} CA biotech" using brand_token_from_etld1()
- Call Text Search → get top 3-5 candidates
- For each candidate, call Place Details with fields: ['name','formatted_address','website','types','geometry','business_status']
- Apply hard gates:
  * Geofence: address must be in Bay Area (use geography module)
  * Business type: exclude real_estate_agency, lodging, premise-only
  * Multi-tenant: if is_multi_tenant(address), require validate_multi_tenant_match()
- Website cross-check: score += 0.3 if details.website eTLD+1 == BPG eTLD+1
- Accept if score ≥ 0.75 (deterministic threshold)
- Keep BPG Website (canonical); add Address, Place_ID, Confidence_Det, Validation_Reason

**Concrete Changes:**
- EDIT: `scripts/enrich_with_google_maps.py`
  - Import from config.geography import is_in_bay_area
  - Import from utils.helpers import etld1, brand_token_from_etld1, name_similarity, is_multi_tenant, validate_multi_tenant_match
  - Replace "first result wins" logic with validate_candidate() function
  - Add calculate_confidence_score() function
  - Add EXCLUDED_BUSINESS_TYPES constant
  - Add Confidence_Det, Place_ID, Validation_Reason columns to output

**Test Plan:**
- Unit: Test brand_token extraction (gene.com → gene, biorad.com → biorad)
- Unit: Test business-type exclusion (real_estate_agency rejected)
- Unit: Test multi-tenant threshold (incubator address requires strong brand match)
- Unit: Test confidence scoring (website match +0.3, geofence +0.2, etc.)
- Integration: Run on red-team clear wins (Genentech, Gilead, BioMarin)
- Integration: Verify score ≥ 0.75 for all clear wins
- Red-team: Test incubator addresses reject without brand match

**Rollback Plan:**
- Revert scripts/enrich_with_google_maps.py
- Delete data/working/companies_enriched.csv if exists
- No impact on data/final/

**Estimated Hours:** 4h

**Labels:** V4.3, Stage-C PathA

---

### Issue #12: Add cost instrumentation and caching for Google Places API

**Title:** [Stage-C PathA] Add API usage counters and Place Details caching

**Rationale:** V4.3 pricing baseline: Text Search $0.032 + Details $0.017 ≈ $0.049/company. Need to measure actual usage to stay within $200/month budget and optimize caching.

**Acceptance Criteria:**
- Count API calls: text_search_calls, place_details_calls
- Log per-run: total companies, Path A companies, avg calls/company, estimated cost
- Cache Place Details by place_id (in-memory during run, disk for re-runs)
- Cache format: `data/cache/place_details_YYYYMMDD.json`
- Cache expiration: 30 days
- Report: api_usage_report.txt with call counts and cost estimate

**Concrete Changes:**
- EDIT: `scripts/enrich_with_google_maps.py`
  - Add APIUsageCounter class
  - Add cache_place_details() and load_cached_details() functions
  - Add generate_api_usage_report() function
  - Add GOOGLE_MAPS_COST_PER_TEXT_SEARCH, GOOGLE_MAPS_COST_PER_DETAILS constants
- ADD: `data/working/api_usage_report.txt` (generated output)
- ADD: `data/cache/place_details_*.json`

**Test Plan:**
- Unit: Test counter increments correctly
- Unit: Test cache saves and loads place_id → details mapping
- Unit: Test cache expiration after 30 days
- Unit: Test cost calculation (calls × pricing)
- Integration: Verify report generated after enrichment run
- Integration: Second run uses cache, reduces API calls

**Rollback Plan:** Remove instrumentation code; core enrichment still works

**Estimated Hours:** 2h

**Labels:** V4.3, Stage-C PathA

---

### Issue #13: Implement error handling, retries, and checkpointing

**Title:** [Stage-C PathA] Add exponential backoff, rate limiting, and progress checkpoints

**Rationale:** Long-running enrichment jobs need resilience: handle API errors, rate limits, network timeouts. Checkpointing enables resuming from failure point.

**Acceptance Criteria:**
- HTTP/API errors: retry 3× with exponential backoff (0.5s, 1s, 2s)
- 429 rate limit: back off 60s, then resume
- Network timeout: retry; if repeated, skip row and log to manual queue
- Checkpoint every 50 rows to `data/working/.checkpoint_enrichment.json`
- Resume from checkpoint if exists (--resume flag)
- Manual queue: `data/working/manual_review_queue.csv` for failed enrichments

**Concrete Changes:**
- EDIT: `scripts/enrich_with_google_maps.py`
  - Add retry_with_backoff() decorator
  - Add save_checkpoint() and load_checkpoint() functions
  - Add --resume CLI argument
  - Add manual queue CSV writer
  - Add rate limiter (delay between requests)
- ADD: `data/working/.checkpoint_enrichment.json`
- ADD: `data/working/manual_review_queue.csv`

**Test Plan:**
- Unit: Test retry logic with mock HTTP errors
- Unit: Test backoff timing (0.5s, 1s, 2s)
- Unit: Test checkpoint save/load with mock data
- Integration: Simulate 429 error, verify 60s backoff
- Integration: Kill process mid-run, verify resume from checkpoint

**Rollback Plan:** Remove error handling code; basic enrichment works but less resilient

**Estimated Hours:** 2h

**Labels:** V4.3, Stage-C PathA

---

### Issue #14: Add Path A/Path B routing logic

**Title:** [Stage-C PathA] Add routing: Website present → Path A, Website absent → Path B

**Rationale:** Path A is for companies with BPG Website (ground truth). Path B is for companies without Website. Need routing logic and placeholder for Path B integration.

**Acceptance Criteria:**
- Read companies_merged.csv
- If Website != '': process with Path A validation
- If Website == '': write to `data/working/path_b_queue.csv` for Path B processing
- Log: Path A count, Path B count, success/failure breakdown
- Output: `data/working/companies_enriched_path_a.csv` (partial, Path A only)

**Concrete Changes:**
- EDIT: `scripts/enrich_with_google_maps.py`
  - Add route_to_path() function
  - Separate Path A processing loop
  - Write Path B queue for later processing
- ADD: `data/working/path_b_queue.csv`
- ADD: `data/working/companies_enriched_path_a.csv`

**Test Plan:**
- Unit: Test routing logic (Website present/absent)
- Integration: Verify Path A rows in enriched_path_a.csv
- Integration: Verify Path B rows in path_b_queue.csv
- Integration: Verify counts match (Path A + Path B = merged total)

**Rollback Plan:** Revert routing logic; process all rows with Path A (some fail)

**Estimated Hours:** 1h

**Labels:** V4.3, Stage-C PathA

---

## Phase 3: PR-4 Path B Structured Outputs [Stage-C PathB] (11 hours)

**Dependencies:** Phase 0 (Issues #1, #2), PR-2 (Issues #7-10), PR-3 (Issue #14 for path_b_queue.csv)

**Note:** Can develop in parallel with PR-3 after PR-2 completes

### Issue #15: Implement Anthropic tool definitions and Python wrappers

**Title:** [Stage-C PathB] Create Google Places tool definitions and Python wrappers for Anthropic

**Rationale:** Path B needs Claude to call Google Places APIs via tool use. Define search_places and get_place_details tools with proper schemas; implement Python wrappers that execute actual API calls.

**Acceptance Criteria:**
- Tool definition: search_places with query, location_bias parameters
- Tool definition: get_place_details with place_id parameter
- Python wrapper: search_places_tool(query, location_bias=None) → JSON response
- Python wrapper: get_place_details_tool(place_id) → JSON with name, formatted_address, website, types, business_status, geometry
- Both wrappers use existing googlemaps.Client instance
- Tool registry: TOOL_REGISTRY = {"search_places": search_places_tool, "get_place_details": get_place_details_tool}

**Concrete Changes:**
- ADD: `scripts/path_b_enrichment.py` (new script for Path B)
  - Define SEARCH_PLACES_TOOL, GET_PLACE_DETAILS_TOOL dictionaries
  - Implement search_places_tool() function
  - Implement get_place_details_tool() function
  - Add TOOL_REGISTRY mapping
- ADD: `requirements.txt` entry: anthropic

**Test Plan:**
- Unit: Test tool definitions match V4.3 schema
- Unit: Test search_places_tool() with mock query returns candidate list
- Unit: Test get_place_details_tool() with mock place_id returns details
- Integration: Call tools with real Google Maps API, verify responses

**Rollback Plan:** Delete scripts/path_b_enrichment.py; Path A still works independently

**Estimated Hours:** 2h

**Labels:** V4.3, Stage-C PathB

---

### Issue #16: Implement structured outputs JSON schema and validation prompt

**Title:** [Stage-C PathB] Define company_enrichment_result schema and validation prompt template

**Rationale:** Structured outputs require strict JSON schema. Define schema matching V4.3 spec; create comprehensive prompt with hard gates (9-county, brand-domain, business-type, multi-tenant, acceptance ≥0.75).

**Acceptance Criteria:**
- JSON schema: company_enrichment_result with required fields (company_name, confidence, validation)
- Schema enforces: address/city/website nullable, confidence [0,1], validation object with booleans + reasoning
- Prompt template includes:
  * Geographic scope (9-county hard gate)
  * Brand-domain validation (reject aggregators)
  * Business-type validation (exclude real_estate, lodging, premise)
  * Multi-tenant handling (incubator addresses need strong brand)
  * Acceptance threshold ≥0.75
  * Prefer nulls over wrong data
- Prompt uses company_name and city hint as variables

**Concrete Changes:**
- ADD: `scripts/path_b_enrichment.py`
  - Define COMPANY_ENRICHMENT_SCHEMA constant (JSON schema dict)
  - Define VALIDATION_PROMPT_TEMPLATE constant (multi-paragraph prompt)
  - Add schema validator: validate_enrichment_result(result) → bool

**Test Plan:**
- Unit: Test schema accepts valid enrichment result
- Unit: Test schema rejects invalid enrichment result (missing fields, wrong types)
- Unit: Test prompt template renders with company_name and city
- Integration: Verify schema matches V4.3 specification exactly

**Rollback Plan:** Delete schema/prompt code; no external impact

**Estimated Hours:** 2h

**Labels:** V4.3, Stage-C PathB

---

### Issue #17: Implement Anthropic messages API controller loop with tool use

**Title:** [Stage-C PathB] Create tool use controller loop with structured outputs and temperature=0

**Rationale:** V4.3 specifies complete controller loop: create message with tools + response_format, handle tool_use blocks, execute Python wrappers, append tool_result, loop up to 8 rounds, extract final JSON.

**Acceptance Criteria:**
- Function: run_structured_enrichment(company_name, city) → dict
- Use Anthropic client with model="claude-3-5-sonnet-20241022"
- Set temperature=0, max_tokens=1200
- Pass tools=[SEARCH_PLACES_TOOL, GET_PLACE_DETAILS_TOOL]
- Pass response_format={"type":"json_schema", "json_schema": COMPANY_ENRICHMENT_SCHEMA}
- Detect tool_use blocks in response.content
- Execute tools via TOOL_REGISTRY, build tool_result blocks
- Loop max 8 iterations
- Extract final JSON from response.content (handle both .text and typed blocks)
- Raise RuntimeError if no JSON found or loop exceeds max

**Concrete Changes:**
- ADD: `scripts/path_b_enrichment.py`
  - Implement run_structured_enrichment() function
  - Add tool execution loop
  - Add JSON extraction logic
  - Add error handling for missing ANTHROPIC_API_KEY

**Test Plan:**
- Unit: Test tool_use detection logic
- Unit: Test tool_result formatting
- Unit: Test JSON extraction from various response formats
- Integration: Run on known company with Website, verify tool calls and final JSON
- Integration: Verify temperature=0 (deterministic)
- Red-team: Test on "no-website" company, verify schema-conforming output

**Rollback Plan:** Delete controller loop; Path A unaffected

**Estimated Hours:** 3h

**Labels:** V4.3, Stage-C PathB

---

### Issue #18: Add Path B acceptance logic and cost instrumentation

**Title:** [Stage-C PathB] Implement acceptance threshold, null handling, and Anthropic cost tracking

**Rationale:** Accept Path B results only if confidence ≥0.75 AND in_bay_area=True AND is_business=True AND not aggregator. Track Anthropic token usage and cost for budget monitoring.

**Acceptance Criteria:**
- Parse result from run_structured_enrichment()
- Accept if: result.confidence ≥ 0.75 AND result.validation.in_bay_area AND result.validation.is_business AND not is_aggregator(result.website)
- If rejected: keep nulls, log to manual_review_queue.csv, record validation.reasoning
- If accepted: add Website, Address, City from result; set Confidence=result.confidence, Validation_Source="PathB", Validation_JSON=json.dumps(result.validation)
- Track Anthropic usage: prompt_tokens, completion_tokens, total_tokens per call
- Generate anthropic_usage_report.txt: total calls, total tokens, estimated cost (tokens × rate)
- Note in report: "Anthropic cost = TBD; instrument actual token pricing from API response"

**Concrete Changes:**
- ADD: `scripts/path_b_enrichment.py`
  - Implement accept_enrichment_result() function
  - Add AnthropicUsageCounter class
  - Add generate_anthropic_usage_report() function
  - Extract usage from response object (response.usage.input_tokens, response.usage.output_tokens)
- ADD: `data/working/anthropic_usage_report.txt`

**Test Plan:**
- Unit: Test acceptance logic (pass/fail scenarios)
- Unit: Test null preservation when rejected
- Unit: Test token counter increments
- Unit: Test aggregator detection in Path B results
- Integration: Verify rejected results in manual queue
- Integration: Verify usage report generated with token counts

**Rollback Plan:** Remove acceptance logic; all Path B results accepted (risky, not recommended)

**Estimated Hours:** 2h

**Labels:** V4.3, Stage-C PathB

---

### Issue #19: Integrate Path B with main enrichment pipeline and merge outputs

**Title:** [Stage-C PathB] Process path_b_queue.csv and merge with Path A results

**Rationale:** Path A produces companies_enriched_path_a.csv; Path B processes path_b_queue.csv. Need to merge both into single companies_enriched.csv for downstream stages.

**Acceptance Criteria:**
- Read `data/working/path_b_queue.csv` (from PR-3 Issue #14)
- Process each row with run_structured_enrichment()
- Apply acceptance logic
- Output: `data/working/companies_enriched_path_b.csv`
- Merge companies_enriched_path_a.csv + companies_enriched_path_b.csv → companies_enriched.csv
- Verify: no duplicate Company Names in final merged output
- Log: Path B processed, Path B accepted, Path B rejected, combined total

**Concrete Changes:**
- ADD: `scripts/path_b_enrichment.py`
  - Add main() function with CLI argument parsing
  - Add CSV reader for path_b_queue.csv
  - Add CSV writer for companies_enriched_path_b.csv
  - Add progress logging (every 10 rows)
- ADD: `scripts/merge_enrichment_outputs.py` (new script)
  - Read path_a and path_b CSVs
  - Concatenate and deduplicate
  - Write to companies_enriched.csv
- ADD: `data/working/companies_enriched.csv` (final Stage C output)

**Test Plan:**
- Integration: Process path_b_queue with multiple companies
- Integration: Verify merged output has all columns (Company Name, Website, Address, City, Confidence, Place_ID, Validation_Source, Validation_JSON)
- Integration: Verify no duplicates in merged output
- Red-team: Run on full red-team set (30 companies), verify ≥90% Tier 1-3

**Rollback Plan:** Delete path_b scripts; use only Path A results (incomplete)

**Estimated Hours:** 2h

**Labels:** V4.3, Stage-C PathB

---

## Phase 4: Stages D/E/F (11 hours)

**Dependencies:** PR-3 and PR-4 (companies_enriched.csv)

### Issue #20: Implement Company Stage classification (Stage D)

**Title:** [Stage-D] Add Company Stage classification using methodology decision tree

**Rationale:** V4.3 requires separate classification pass using methodology's 8 categories. Avoid mixing with enrichment; leave "Unknown" if ambiguous; don't hallucinate.

**Acceptance Criteria:**
- Read companies_enriched.csv
- Apply methodology decision tree: Public, Private (Series A-D+), Acquired, Clinical, Research, Incubator, Service, Unknown
- Use verifiable signals: stock ticker, funding announcements, clinical trials, pipeline stage
- Default to "Unknown" if ambiguous (prefer null over wrong)
- Add Company_Stage column
- Add Classifier_Date column (YYYY-MM-DD)
- Output: `data/working/companies_classified.csv`

**Concrete Changes:**
- ADD: `scripts/classify_company_stage.py` (new script)
  - Implement classify_company_stage(company_name, website) → stage
  - Add methodology decision tree logic
  - Add --manual flag for human review mode
- EDIT: `requirements.txt` (if using external data sources)

**Test Plan:**
- Unit: Test classification logic for known categories
- Unit: Test "Unknown" default for ambiguous cases
- Integration: Classify red-team set, verify Public companies (Genentech, Gilead) correct
- Manual: Review 10 random classifications for accuracy

**Rollback Plan:**
- Delete scripts/classify_company_stage.py
- Use companies_enriched.csv without classification

**Estimated Hours:** 3h

**Labels:** V4.3, Stage-D

---

### Issue #21: Implement Focus Areas extraction (Stage E)

**Title:** [Stage-E] Extract factual Focus Areas from company websites

**Rationale:** V4.3 requires 1-3 factual sentences (≤200 chars) from About/Technology pages; de-marketed; include platform keywords.

**Acceptance Criteria:**
- Read companies_classified.csv
- For each company with Website: fetch homepage, find About/Technology section
- Extract 1-3 plain sentences (avoid marketing fluff)
- Maximum 200 characters
- Include keywords: platform, technology, therapeutic area, modality
- Add Focus_Areas column
- Respect rate limits, cache HTML
- Default to empty string if extraction fails
- Output: `data/working/companies_focused.csv`

**Concrete Changes:**
- ADD: `scripts/extract_focus_areas.py` (new script)
  - Implement fetch_and_extract_focus(website) → focus_text
  - Add HTML caching (reuse BPG cache directory)
  - Add rate limiting (1 request/second)
  - Add timeout handling (5s per request)
- Optional: Use Claude for extraction if needed

**Test Plan:**
- Unit: Test HTML parsing and text extraction
- Unit: Test 200-char limit enforcement
- Integration: Extract focus for 10 known companies
- Manual: Review 10 random Focus Areas for quality

**Rollback Plan:**
- Delete scripts/extract_focus_areas.py
- Leave Focus_Areas empty in final output

**Estimated Hours:** 3h

**Labels:** V4.3, Stage-E

---

### Issue #22: Implement automated validators (Stage F - Part 1)

**Title:** [Stage-F QC] Create automated validators for promotion gate

**Rationale:** V4.3 requires passing all validators before promotion. Check: valid URLs, city whitelist, Bay Area geofence, no duplicate eTLD+1, no aggregators, Place_ID if Address present.

**Acceptance Criteria:**
- Validator: validate_urls() - all Website fields valid HTTPS or blank
- Validator: validate_geofence() - all City in whitelist, all Address in Bay Area
- Validator: validate_no_duplicate_domains() - zero duplicate eTLD+1 (except allowlist)
- Validator: validate_no_aggregators() - no aggregator domains in final output
- Validator: validate_place_ids() - if Address present, Place_ID present
- Validator: validate_no_out_of_scope() - zero Davis/Sacramento/out-of-area companies
- All validators must PASS (exit code 0) to proceed to promotion
- Generate validation_report.txt with pass/fail results

**Concrete Changes:**
- ADD: `scripts/validate_for_promotion.py` (new script)
  - Implement each validator function
  - Add run_all_validators() with reporting
  - Exit with code 1 if any validator fails
- ADD: `data/working/validation_report.txt`

**Test Plan:**
- Unit: Test each validator with mock data (pass and fail scenarios)
- Integration: Run validators on companies_focused.csv
- Integration: Inject known violations, verify detection
- Red-team: Validators pass for red-team set

**Rollback Plan:** Delete validator script; manual review only (risky)

**Estimated Hours:** 2h

**Labels:** V4.3, Stage-F QC

---

### Issue #23: Implement manual review and spot checks (Stage F - Part 2)

**Title:** [Stage-F QC] Add manual review queue and spot-check sampling

**Rationale:** V4.3 requires manual spot-checks: 10 random Tier 1/2 (address matches website), all Tier 4 (flagged) reviewed.

**Acceptance Criteria:**
- Calculate tiers based on Confidence:
  * Tier 1: Confidence ≥ 0.95 (BPG + Google confirm)
  * Tier 2: Confidence 0.90-0.95 (BPG only)
  * Tier 3: Confidence 0.75-0.90 (AI validated)
  * Tier 4: Confidence < 0.75 (flagged)
- Generate spot_check_sample.csv: 10 random Tier 1/2
- Generate tier_4_review.csv: all Tier 4 companies
- Manual review instructions: verify address on company website
- After manual review, mark reviewed companies in reviewed_flags.csv
- Block promotion if Tier 4 unreviewed

**Concrete Changes:**
- ADD: `scripts/generate_review_queues.py` (new script)
  - Implement calculate_tier() function
  - Generate spot_check_sample.csv (random 10 from Tier 1/2)
  - Generate tier_4_review.csv (all Tier 4)
- ADD: `data/working/spot_check_sample.csv`
- ADD: `data/working/tier_4_review.csv`
- ADD: `data/working/reviewed_flags.csv` (manual input)

**Test Plan:**
- Unit: Test tier calculation logic
- Integration: Generate review queues from companies_focused.csv
- Integration: Verify sample size correct (10 random)
- Manual: Perform actual spot-checks and Tier 4 review

**Rollback Plan:** Skip manual review; promote with automated validation only (lower quality)

**Estimated Hours:** 1h (scripting) + variable manual review time

**Labels:** V4.3, Stage-F QC

---

### Issue #24: Implement promotion script (Stage F - Part 3)

**Title:** [Stage-F QC] Create promotion script to write data/final/companies.csv

**Rationale:** Only after all validators pass + manual reviews complete, promote from working/ to final/. This is the ONLY script that writes to data/final/.

**Acceptance Criteria:**
- Check: validation_report.txt shows all PASS
- Check: reviewed_flags.csv exists and covers all Tier 4
- Read companies_focused.csv (working)
- Select final columns: Company Name, Website, City, Address, Company Stage, Focus Areas
- Drop working columns: Confidence, Confidence_Det, Place_ID, Validation_Source, Validation_JSON, Last_Verified
- Write to `data/final/companies.csv`
- Add metadata: `data/final/last_updated.txt` with timestamp and stats
- Log promotion: total companies, tier breakdown, coverage stats
- Prevent promotion if validators failed or Tier 4 unreviewed

**Concrete Changes:**
- ADD: `scripts/promote_to_final.py` (new script)
  - Check validation_report.txt
  - Check reviewed_flags.csv completeness
  - Select production columns
  - Write to data/final/companies.csv
  - Write metadata to data/final/last_updated.txt
- EDIT: `data/final/companies.csv` (production output)
- ADD: `data/final/last_updated.txt`

**Test Plan:**
- Unit: Test column selection logic
- Unit: Test promotion blocking if validators fail
- Integration: Run full pipeline end-to-end on red-team set
- Integration: Verify final output schema matches README
- Validation: Confirm ≥70% Tier 1, ≤10% Tier 3, 0% Tier 4 in final

**Rollback Plan:**
- Restore previous data/final/companies.csv from git
- Delete last_updated.txt
- Working files unchanged

**Estimated Hours:** 2h

**Labels:** V4.3, Stage-F QC

---

## Phase 5: Documentation (4 hours)

**Dependencies:** All previous issues (need complete picture for docs)

### Issue #25: Update METHODOLOGY.md with V4.3 validation strategy

**Title:** [Docs] Add Validation Strategy and Confidence Tiers to METHODOLOGY.md

**Rationale:** V4.3 framework introduces new validation logic (geofence hard gate, brand-domain check, business-type filter, multi-tenant threshold, Path A/B split). Document these in methodology for transparency.

**Acceptance Criteria:**
- Add "Validation Strategy" section:
  * Geofence: 9-county hard gate (list counties)
  * Brand-domain check: reject aggregators, verify eTLD+1 match
  * Business-type filter: exclude real_estate, lodging, premise-only
  * Multi-tenant threshold: incubator addresses need strong brand evidence
  * Path A vs Path B: Website present → Path A (pure Python), absent → Path B (structured outputs)
  * Acceptance threshold: ≥0.75 confidence
  * Prefer nulls over wrong data
- Add "Confidence Tiers" section:
  * Tier 1 (≥0.95): BPG + Google confirm same eTLD+1
  * Tier 2 (0.90-0.95): BPG ground truth, Google mismatch/missing
  * Tier 3 (0.75-0.90): AI validated (Path B)
  * Tier 4 (<0.75): Flagged for manual review
- Add "QC Checklist" section: automated validators, manual spot-checks, promotion gates
- Update field names: use "Focus Areas" not "Notes"

**Concrete Changes:**
- EDIT: `METHODOLOGY.md`
  - Add Validation Strategy section after Geographic Scope
  - Add Confidence Tiers section before Data Sources
  - Add QC Checklist section at end
  - Fix field name inconsistencies

**Test Plan:**
- Manual: Review for clarity and completeness
- Validation: Cross-reference with V4.3 framework

**Rollback Plan:** Revert METHODOLOGY.md to previous version

**Estimated Hours:** 1h

**Labels:** V4.3, docs

---

### Issue #26: Update README.md with accurate counts and V4.3 features

**Title:** [Docs] Update README with V4.3 pipeline, coverage stats, and data quality section

**Rationale:** Current README has inconsistent counts/coverage claims. Update with accurate stats from V4.3 implementation; add Data Quality section describing tiers and gates.

**Acceptance Criteria:**
- Update "Features" section: mention two-path enrichment (BPG-first, AI fallback)
- Update "Data Quality" section (new):
  * Tier breakdown (target: ≥70% Tier 1, ≥10% Tier 2, ≤10% Tier 3, 0% Tier 4)
  * Validation gates (geofence, no duplicates, no aggregators)
  * Manual review process (spot-checks + Tier 4)
- Update "Pipeline Overview": reflect A→F stages
- Fix field names: "Focus Areas" everywhere (not "Notes")
- Update coverage claims only after full V4.3 run (placeholder: "Updated YYYY-MM-DD")
- Add "API Costs" section: Google Maps pricing ($0.049/company baseline), Anthropic measured empirically

**Concrete Changes:**
- EDIT: `README.md`
  - Update Features section
  - Add Data Quality section
  - Update Pipeline Overview
  - Fix field names
  - Add API Costs section
  - Add placeholder for coverage stats (fill after run)

**Test Plan:**
- Manual: Review for clarity and accuracy
- Validation: Verify field names consistent across docs

**Rollback Plan:** Revert README.md to previous version

**Estimated Hours:** 1h

**Labels:** V4.3, docs

---

### Issue #27: Create V4.3 implementation guide and runbook

**Title:** [Docs] Create docs/V4.3_IMPLEMENTATION_GUIDE.md with step-by-step execution instructions

**Rationale:** Provide clear runbook for executing V4.3 pipeline: order of operations, required API keys, expected outputs, troubleshooting.

**Acceptance Criteria:**
- Document: `docs/V4.3_IMPLEMENTATION_GUIDE.md`
- Sections:
  * Prerequisites (Python, API keys, dependencies)
  * Phase 0: Infrastructure setup (run tests)
  * Phase 1: Extraction (run BPG extractor, verify output)
  * Phase 2: Merge (run merge script, check domain_reuse_report)
  * Phase 3: Enrichment (run Path A, then Path B, merge outputs)
  * Phase 4: Classification & Focus (run both scripts)
  * Phase 5: QC & Promotion (run validators, manual reviews, promote)
  * Troubleshooting: common errors and fixes
  * Cost monitoring: check usage reports
- Include example commands for each phase
- Include expected file outputs at each stage

**Concrete Changes:**
- ADD: `docs/V4.3_IMPLEMENTATION_GUIDE.md` (new file)

**Test Plan:**
- Manual: Follow guide on red-team set, verify completeness
- Validation: Ensure all scripts mentioned exist

**Rollback Plan:** Delete docs/V4.3_IMPLEMENTATION_GUIDE.md

**Estimated Hours:** 2h

**Labels:** V4.3, docs

---

## GitHub Labels

Apply these labels to organize and track issues:

- **V4.3** - All issues (project version)
- **infrastructure** - Phase 0 issues
- **Stage-A** - BPG extractor issues
- **Stage-B** - Merge & geofence issues
- **Stage-C PathA** - Path A enrichment issues
- **Stage-C PathB** - Path B structured outputs issues
- **Stage-D** - Classification issues
- **Stage-E** - Focus areas issues
- **Stage-F QC** - QC & promotion issues
- **docs** - Documentation issues
- **testing** - Test-related issues
- **blocked** - Waiting on dependencies
- **needs-decision** - Requires clarification or decision

---

## Success Metrics

### Tier Distribution (Post-QC)
- **Tier 1:** ≥70% (BPG + Google confirm same eTLD+1)
- **Tier 2:** ≥10% (BPG ground truth, Google mismatch/missing)
- **Tier 3:** ≤10% (AI validated via Path B)
- **Tier 4:** 0% (all flagged items manually reviewed and resolved)

### Data Quality Gates
- ✓ Zero Davis/Sacramento/out-of-scope companies
- ✓ Zero duplicate eTLD+1 (except allowlist)
- ✓ Zero aggregator domains in final output
- ✓ All automated validators PASS
- ✓ All Tier 4 manually reviewed
- ✓ 10 random Tier 1/2 spot-checked

### Reproducibility
- ✓ Red-team set achieves ≥90% Tier 1-3
- ✓ Path B outcomes variance ≤5% (with same seeds and caches)
- ✓ All tests passing (unit, integration, validation)

### Budget
- ✓ Google Maps API costs within $200/month credit
- ✓ Anthropic costs measured and reported

---

## Critical Path & Parallelization

**Sequential Critical Path:**
```
Phase 0 → PR-1 → PR-2 → (PR-3 || PR-4) → Stages D/E/F → Docs
```

**Parallelization Opportunities:**
1. **PR-3 and PR-4** can develop concurrently after PR-2 completes
2. **Documentation** (Phase 5) can start anytime but finishes last
3. **Testing** integrates throughout all phases

**Timeline Estimates:**
- Sequential execution: ~53 hours
- With parallelization (PR-3 || PR-4): ~42 hours
- Plus manual review time (variable)

---

## Implementation Notes

### Process Rules
1. **Follow A→F staging order** for data flow
2. **Minimize scope per PR**; keep changes reversible
3. **No writes to data/final/** during enrichment; only promotion step writes there
4. **Test on red-team set** before full production run
5. **Instrument costs** early to avoid budget overruns

### First Four PRs (As Specified)
1. **PR-1:** BPG Extractor (Issues #4-6)
2. **PR-2:** Merge/Geofence (Issues #7-10)
3. **PR-3:** Path A Enrichment (Issues #11-14)
4. **PR-4:** Path B Structured Outputs (Issues #15-19)

### Decision Points
- **Path A scoring too strict/loose?** → Tune thresholds on red-team set before full run
- **Path B costs exceed budget?** → Reduce batch size or add human review threshold
- **Duplicate eTLD+1 issues persist?** → Enhance deduplication logic or manual curation

---

## Next Steps

1. **Review this plan** with stakeholders
2. **Create GitHub issues** from this work plan (27 issues)
3. **Set up project board** with Phase 0, 1, 2, 3, 4, 5 columns
4. **Begin Phase 0** implementation (infrastructure)
5. **Run red-team validation** after each phase
6. **Monitor costs** throughout implementation

---

**End of V4.3 Work Plan**
